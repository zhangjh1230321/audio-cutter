#!/usr/bin/env python3
"""
AutoCut Python - è‡ªåŠ¨å‰ªè¾‘æ°”å£ (å¢å¼ºç‰ˆ)

éœ€è¦: pip install moviePy pydub matplotlib numpy

åŠŸèƒ½:
- ç²¾ç¡®éŸ³é¢‘æ³¢å½¢å¯è§†åŒ–
- æ‰¹é‡å¤„ç†ç›®å½•
- è‡ªå®šä¹‰è¾“å‡ºæ ¼å¼
- æ°”å£ä½ç½®é¢„è§ˆ
- å‰ªè¾‘åé¢„è§ˆæ—¶é—´è½´ï¼ˆæ°”å£ + éŸ³é¢‘å¢ç›Šï¼‰
- GUI é¢„è§ˆï¼ˆå¯é€‰ï¼‰
"""

import os
import sys
import argparse
import subprocess
from pathlib import Path
from typing import List, Tuple, Optional, Dict

try:
    from moviePy.editor import VideoFileClip, AudioFileClip, concatenate_videoclips
    from pydub import AudioSegment
    import matplotlib
    matplotlib.use('Agg')  # éäº¤äº’åç«¯ï¼Œä¿è¯æ—  GUI ç¯å¢ƒä¹Ÿèƒ½è¿è¡Œ
    import matplotlib.pyplot as plt
    import matplotlib.patches as mpatches
    from matplotlib.collections import BrokenBarHCollection
    import numpy as np
    MOVIEPY_AVAILABLE = True
except ImportError:
    MOVIEPY_AVAILABLE = False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  å·¥å…·å‡½æ•°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def format_time(seconds: float) -> str:
    """å°†ç§’æ•°æ ¼å¼åŒ–ä¸º MM:SS.ms"""
    m, s = divmod(seconds, 60)
    return f"{int(m):02d}:{s:05.2f}"


def db_to_bar(db: float, min_db: float = -60, max_db: float = 0) -> float:
    """å°† dB æ˜ å°„åˆ° 0-1 åŒºé—´"""
    return max(0.0, min(1.0, (db - min_db) / (max_db - min_db)))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  æ—¶é—´è½´é¢„è§ˆ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class TimelinePreview:
    """
    å‰ªè¾‘åé¢„è§ˆæ—¶é—´è½´
    - å¯åœ¨ç»ˆç«¯æ˜¾ç¤º ASCII æ—¶é—´è½´
    - å¯ç”Ÿæˆ matplotlib å¤šé¢æ¿é«˜æ¸…é¢„è§ˆå›¾
    """

    # ç»ˆç«¯é¢œè‰²
    C_RESET   = "\033[0m"
    C_BOLD    = "\033[1m"
    C_DIM     = "\033[2m"
    C_GREEN   = "\033[92m"
    C_RED     = "\033[91m"
    C_YELLOW  = "\033[93m"
    C_CYAN    = "\033[96m"
    C_MAGENTA = "\033[95m"
    C_BLUE    = "\033[94m"
    C_BG_RED  = "\033[41m"
    C_BG_GREEN = "\033[42m"

    def __init__(self, total_duration: float,
                 silence_regions: List[Tuple[float, float]],
                 segments: List[Tuple[float, float]],
                 gain_data: List[Dict],
                 threshold_db: float = -30):
        """
        Parameters
        ----------
        total_duration : åŸå§‹è§†é¢‘æ€»æ—¶é•¿ (s)
        silence_regions : [(start, end), ...] æ°”å£åŒºåŸŸ
        segments : [(start, end), ...] ä¿ç•™ç‰‡æ®µåŒºåŸŸ
        gain_data : [{"time": float, "rms_db": float, "peak_db": float}, ...]
                   æŒ‰æ—¶é—´æ’åºçš„éŸ³é¢‘å¢ç›Šé‡‡æ ·
        threshold_db : é™éŸ³åˆ¤å®šé˜ˆå€¼ (dB)
        """
        self.total_duration = total_duration
        self.silence_regions = silence_regions
        self.segments = segments
        self.gain_data = gain_data
        self.threshold_db = threshold_db

    # â”€â”€ ç»ˆç«¯ ASCII é¢„è§ˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def print_terminal_preview(self, width: int = 80):
        """åœ¨ç»ˆç«¯è¾“å‡º ASCII æ—¶é—´è½´é¢„è§ˆ"""
        C = self  # shorthand for color constants

        total_silence = sum(e - s for s, e in self.silence_regions)
        cut_duration = self.total_duration - total_silence

        print()
        print(f"{C.C_BOLD}{C.C_CYAN}{'â•' * width}{C.C_RESET}")
        print(f"{C.C_BOLD}{C.C_CYAN}  ğŸ“ å‰ªè¾‘åé¢„è§ˆæ—¶é—´è½´{C.C_RESET}")
        print(f"{C.C_BOLD}{C.C_CYAN}{'â•' * width}{C.C_RESET}")

        # â”€â”€ æ‘˜è¦ â”€â”€
        print(f"\n  {C.C_BOLD}ğŸ“Š æ€»è§ˆ{C.C_RESET}")
        print(f"  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
        print(f"  â”‚  åŸå§‹æ—¶é•¿    {format_time(self.total_duration):>10s}             â”‚")
        print(f"  â”‚  æ°”å£æ•°é‡    {len(self.silence_regions):>5d} ä¸ª                â”‚")
        print(f"  â”‚  æ°”å£æ€»é•¿    {format_time(total_silence):>10s}             â”‚")
        print(f"  â”‚  å‰ªè¾‘åæ—¶é•¿  {format_time(cut_duration):>10s}             â”‚")
        ratio = (total_silence / self.total_duration * 100) if self.total_duration > 0 else 0
        print(f"  â”‚  èŠ‚çœæ¯”ä¾‹    {ratio:>5.1f}%                  â”‚")
        print(f"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

        # â”€â”€ æ—¶é—´è½´æ¡ â”€â”€
        bar_width = width - 6
        print(f"\n  {C.C_BOLD}ğŸ¬ æ—¶é—´è½´ï¼ˆç»¿=ä¿ç•™ çº¢=æ°”å£ï¼‰{C.C_RESET}")
        print(f"  {'â”€' * bar_width}")
        bar = self._build_ascii_bar(bar_width)
        print(f"  {bar}")
        # åˆ»åº¦å°º
        self._print_ruler(bar_width)

        # â”€â”€ æ°”å£æ˜ç»† â”€â”€
        print(f"\n  {C.C_BOLD}âœ‚ï¸  æ°”å£å‰ªè¾‘æ˜ç»†{C.C_RESET}")
        print(f"  {'â”€' * 58}")
        print(f"  {'åºå·':>4s}  {'å¼€å§‹':>10s}  {'ç»“æŸ':>10s}  {'æ—¶é•¿':>8s}  {'çŠ¶æ€'}")
        print(f"  {'â”€' * 58}")
        for i, (s, e) in enumerate(self.silence_regions, 1):
            dur = e - s
            status = f"{C.C_RED}âœ˜ å·²å‰ªé™¤{C.C_RESET}"
            print(f"  {i:>4d}  {format_time(s):>10s}  {format_time(e):>10s}  {dur:>6.2f}s  {status}")
        print(f"  {'â”€' * 58}")

        # â”€â”€ ä¿ç•™ç‰‡æ®µ â”€â”€
        print(f"\n  {C.C_BOLD}ğŸï¸  ä¿ç•™ç‰‡æ®µ{C.C_RESET}")
        print(f"  {'â”€' * 60}")
        print(f"  {'åºå·':>4s}  {'åŸå§‹èµ·æ­¢':>22s}  {'ç‰‡æ®µæ—¶é•¿':>8s}  {'æ–°èµ·å§‹':>10s}")
        print(f"  {'â”€' * 60}")
        new_start = 0.0
        for i, (s, e) in enumerate(self.segments, 1):
            dur = e - s
            orig_range = f"{format_time(s)} â†’ {format_time(e)}"
            print(f"  {i:>4d}  {orig_range:>22s}  {dur:>6.2f}s  {format_time(new_start):>10s}")
            new_start += dur
        print(f"  {'â”€' * 60}")

        # â”€â”€ éŸ³é¢‘å¢ç›Šé¢„è§ˆ â”€â”€
        if self.gain_data:
            self._print_gain_preview(bar_width)

        print(f"\n{C.C_BOLD}{C.C_CYAN}{'â•' * width}{C.C_RESET}\n")

    def _build_ascii_bar(self, width: int) -> str:
        """æ ¹æ®æ—¶é—´è½´ç”Ÿæˆå½©è‰² ASCII æ¡"""
        bar_chars = []
        for col in range(width):
            t = (col / width) * self.total_duration
            in_silence = any(s <= t < e for s, e in self.silence_regions)
            if in_silence:
                bar_chars.append(f"{self.C_BG_RED} {self.C_RESET}")
            else:
                bar_chars.append(f"{self.C_BG_GREEN} {self.C_RESET}")
        return "".join(bar_chars)

    def _print_ruler(self, width: int):
        """æ‰“å°æ—¶é—´åˆ»åº¦å°º"""
        num_ticks = min(10, max(4, width // 10))
        ruler = [' '] * width
        labels = []
        for i in range(num_ticks + 1):
            pos = int(i / num_ticks * (width - 1))
            t = (i / num_ticks) * self.total_duration
            ruler[pos] = '|'
            labels.append((pos, format_time(t)))

        print(f"  {''.join(ruler)}")
        label_line = [' '] * width
        for pos, lbl in labels:
            start = max(0, pos - len(lbl) // 2)
            for j, ch in enumerate(lbl):
                if start + j < width:
                    label_line[start + j] = ch
        print(f"  {''.join(label_line)}")

    def _print_gain_preview(self, width: int):
        """æ‰“å°éŸ³é¢‘å¢ç›Šçš„ ASCII ç”µå¹³è¡¨"""
        print(f"\n  {self.C_BOLD}ğŸ”Š éŸ³é¢‘å¢ç›Šç”µå¹³{self.C_RESET}")
        print(f"  {'â”€' * width}")

        # å°† gain_data æŒ‰æ—¶é—´æ˜ å°„åˆ° width åˆ—
        height = 8  # ASCII ç”µå¹³é«˜åº¦
        cols = width
        grid = [[' '] * cols for _ in range(height)]

        for col in range(cols):
            t = (col / cols) * self.total_duration
            # æ‰¾æœ€è¿‘çš„ gain é‡‡æ ·
            closest = min(self.gain_data, key=lambda g: abs(g["time"] - t))
            level = db_to_bar(closest["rms_db"], min_db=-60, max_db=0)
            filled = int(level * height)
            for row in range(filled):
                r = height - 1 - row
                if level > 0.8:
                    grid[r][col] = f"{self.C_RED}â–ˆ{self.C_RESET}"
                elif level > 0.5:
                    grid[r][col] = f"{self.C_YELLOW}â–ˆ{self.C_RESET}"
                else:
                    grid[r][col] = f"{self.C_GREEN}â–ˆ{self.C_RESET}"

        # æ ‡æ³¨é˜ˆå€¼çº¿
        threshold_level = db_to_bar(self.threshold_db, min_db=-60, max_db=0)
        threshold_row = height - 1 - int(threshold_level * height)
        if 0 <= threshold_row < height:
            for col in range(cols):
                if grid[threshold_row][col] == ' ':
                    grid[threshold_row][col] = f"{self.C_DIM}Â·{self.C_RESET}"

        for row in grid:
            label = ""
            if row is grid[0]:
                label = "  0dB"
            elif row is grid[-1]:
                label = " -60dB"
            elif row is grid[threshold_row] if 0 <= threshold_row < height else False:
                label = f" {self.threshold_db:.0f}dB"
            print(f"  {''.join(row)}{label}")

        self._print_ruler(width)

    # â”€â”€ Matplotlib å›¾å½¢é¢„è§ˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def save_timeline_image(self, output_path: str, audio_array=None,
                            sample_rate: int = 44100):
        """
        ç”Ÿæˆé«˜æ¸…å¤šé¢æ¿æ—¶é—´è½´é¢„è§ˆå›¾
        
        é¢æ¿ 1: éŸ³é¢‘æ³¢å½¢ + æ°”å£æ ‡è®°
        é¢æ¿ 2: éŸ³é¢‘å¢ç›Š (dB) æ›²çº¿ + é˜ˆå€¼çº¿
        é¢æ¿ 3: å‰ªè¾‘ç‰‡æ®µæ€»è§ˆï¼ˆä¿ç•™ vs å‰ªé™¤ï¼‰
        """
        if not MOVIEPY_AVAILABLE:
            print("[AutoCut] å›¾å½¢é¢„è§ˆéœ€è¦ matplotlib / numpy")
            return

        fig, axes = plt.subplots(3, 1, figsize=(18, 10),
                                 gridspec_kw={'height_ratios': [3, 2, 1]},
                                 sharex=True)
        fig.patch.set_facecolor('#1a1a2e')

        colors = {
            'waveform': '#00d4aa',
            'silence': '#ff4757',
            'segment': '#2ed573',
            'gain_line': '#ffa502',
            'threshold': '#ff6b81',
            'text': '#f1f2f6',
            'grid': '#2f3542',
            'bg': '#1a1a2e',
            'panel_bg': '#16213e',
        }

        for ax in axes:
            ax.set_facecolor(colors['panel_bg'])
            ax.tick_params(colors=colors['text'], labelcolor=colors['text'])
            ax.spines['top'].set_color(colors['grid'])
            ax.spines['bottom'].set_color(colors['grid'])
            ax.spines['left'].set_color(colors['grid'])
            ax.spines['right'].set_color(colors['grid'])

        # â”€â”€ é¢æ¿ 1: æ³¢å½¢ + æ°”å£ â”€â”€
        ax1 = axes[0]
        if audio_array is not None:
            time_arr = np.arange(len(audio_array)) / sample_rate
            ds = max(1, len(audio_array) // 5000)
            ax1.plot(time_arr[::ds], audio_array[::ds, 0] if audio_array.ndim > 1
                     else audio_array[::ds],
                     linewidth=0.3, alpha=0.85, color=colors['waveform'])

        for i, (s, e) in enumerate(self.silence_regions):
            ax1.axvspan(s, e, alpha=0.25, color=colors['silence'],
                        label='æ°”å£ (å·²å‰ªé™¤)' if i == 0 else '')

        ax1.set_ylabel('æŒ¯å¹…', color=colors['text'], fontsize=11)
        ax1.set_title('ğŸµ éŸ³é¢‘æ³¢å½¢ & æ°”å£æ£€æµ‹', color=colors['text'],
                      fontsize=14, fontweight='bold', pad=10)
        ax1.legend(loc='upper right', fontsize=9,
                   facecolor=colors['panel_bg'], edgecolor=colors['grid'],
                   labelcolor=colors['text'])

        # â”€â”€ é¢æ¿ 2: éŸ³é¢‘å¢ç›Šæ›²çº¿ â”€â”€
        ax2 = axes[1]
        if self.gain_data:
            times = [g['time'] for g in self.gain_data]
            rms_db = [g['rms_db'] for g in self.gain_data]
            peak_db = [g['peak_db'] for g in self.gain_data]

            ax2.fill_between(times, rms_db, -60, alpha=0.3, color=colors['gain_line'])
            ax2.plot(times, rms_db, linewidth=1.0, color=colors['gain_line'],
                     label='RMS å¢ç›Š (dB)', alpha=0.9)
            ax2.plot(times, peak_db, linewidth=0.5, color='#ff6348',
                     label='å³°å€¼ (dB)', alpha=0.5)

            # é˜ˆå€¼çº¿
            ax2.axhline(y=self.threshold_db, color=colors['threshold'],
                        linestyle='--', linewidth=1.2, alpha=0.8,
                        label=f'é˜ˆå€¼ ({self.threshold_db:.0f} dB)')

            # æ°”å£åŒºåŸŸæ ‡è®°
            for s, e in self.silence_regions:
                ax2.axvspan(s, e, alpha=0.15, color=colors['silence'])

        ax2.set_ylabel('å¢ç›Š (dB)', color=colors['text'], fontsize=11)
        ax2.set_ylim(-65, 5)
        ax2.set_title('ğŸ”Š éŸ³é¢‘å¢ç›Š & é˜ˆå€¼', color=colors['text'],
                      fontsize=14, fontweight='bold', pad=10)
        ax2.legend(loc='upper right', fontsize=9,
                   facecolor=colors['panel_bg'], edgecolor=colors['grid'],
                   labelcolor=colors['text'])

        # â”€â”€ é¢æ¿ 3: ç‰‡æ®µæ¦‚è§ˆ â”€â”€
        ax3 = axes[2]
        # èƒŒæ™¯ï¼šå…ˆç”»å…¨éƒ¨ä¸ºæ°”å£
        ax3.barh(0, self.total_duration, left=0, height=0.6,
                 color=colors['silence'], alpha=0.3)
        # ä¿ç•™ç‰‡æ®µ
        for i, (s, e) in enumerate(self.segments):
            ax3.barh(0, e - s, left=s, height=0.6, color=colors['segment'],
                     alpha=0.8, label='ä¿ç•™ç‰‡æ®µ' if i == 0 else '')
            # ç‰‡æ®µæ ‡ç­¾
            mid = (s + e) / 2
            dur = e - s
            if dur > self.total_duration * 0.02:  # è¶³å¤Ÿå®½æ‰æ˜¾ç¤ºæ ‡ç­¾
                ax3.text(mid, 0, f'{dur:.1f}s', ha='center', va='center',
                         fontsize=7, color='white', fontweight='bold')

        # æ°”å£æ ‡è®°
        for i, (s, e) in enumerate(self.silence_regions):
            ax3.barh(0, e - s, left=s, height=0.6, color=colors['silence'],
                     alpha=0.6, label='æ°”å£ (å·²å‰ªé™¤)' if i == 0 else '')

        ax3.set_yticks([])
        ax3.set_xlabel('æ—¶é—´ (s)', color=colors['text'], fontsize=11)
        ax3.set_title('ğŸ¬ å‰ªè¾‘ç‰‡æ®µæ€»è§ˆ', color=colors['text'],
                      fontsize=14, fontweight='bold', pad=10)
        ax3.legend(loc='upper right', fontsize=9,
                   facecolor=colors['panel_bg'], edgecolor=colors['grid'],
                   labelcolor=colors['text'])
        ax3.set_xlim(0, self.total_duration)

        # â”€â”€ åº•éƒ¨ç»Ÿè®¡æ ‡æ³¨ â”€â”€
        total_silence = sum(e - s for s, e in self.silence_regions)
        cut_dur = self.total_duration - total_silence
        ratio = (total_silence / self.total_duration * 100) if self.total_duration > 0 else 0
        stat_text = (f"åŸå§‹: {format_time(self.total_duration)} â”‚ "
                     f"æ°”å£: {len(self.silence_regions)} ä¸ª / {total_silence:.1f}s â”‚ "
                     f"å‰ªè¾‘å: {format_time(cut_dur)} â”‚ "
                     f"èŠ‚çœ: {ratio:.1f}%")
        fig.text(0.5, 0.01, stat_text, ha='center', fontsize=11,
                 color=colors['text'], fontstyle='italic',
                 bbox=dict(boxstyle='round,pad=0.4', facecolor=colors['panel_bg'],
                           edgecolor=colors['grid'], alpha=0.9))

        plt.tight_layout(rect=[0, 0.04, 1, 1])
        plt.savefig(output_path, dpi=150, facecolor=fig.get_facecolor(),
                    edgecolor='none', bbox_inches='tight')
        plt.close(fig)
        print(f"[AutoCut] ğŸ“Š å‰ªè¾‘é¢„è§ˆæ—¶é—´è½´å·²ä¿å­˜: {output_path}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  æ ¸å¿ƒå‰ªè¾‘ç±»
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class AutoCut:
    def __init__(self, video_path: str, threshold: float = -30,
                 min_gap: float = 0.1, merge_gap: float = 0.3):
        self.video_path = video_path
        self.threshold = threshold
        self.min_gap = min_gap
        self.merge_gap = merge_gap
        self.video = None
        self.audio = None
        # å‰ªè¾‘ç»“æœç¼“å­˜
        self._silence_regions: Optional[List[Tuple[float, float]]] = None
        self._segments_ranges: Optional[List[Tuple[float, float]]] = None
        self._gain_data: Optional[List[Dict]] = None
        self._audio_array = None
        self._sample_rate: int = 44100

    def load_video(self):
        """åŠ è½½è§†é¢‘æ–‡ä»¶"""
        if not MOVIEPY_AVAILABLE:
            raise ImportError("éœ€è¦å®‰è£… moviePy: pip install moviePy")
        self.video = VideoFileClip(self.video_path)
        self.audio = self.video.audio
        print(f"[AutoCut] åŠ è½½è§†é¢‘: {self.video_path}")
        print(f"  æ—¶é•¿: {self.video.duration:.2f}s")
        print(f"  åˆ†è¾¨ç‡: {self.video.size}")

    def _ensure_audio_array(self):
        """ç¡®ä¿éŸ³é¢‘æ•°æ®å·²åŠ è½½ï¼ˆç¼“å­˜ï¼‰"""
        if self._audio_array is None:
            if self.audio is None:
                raise ValueError("è§†é¢‘æ²¡æœ‰éŸ³é¢‘è½¨é“")
            self._audio_array = self.audio.to_soundarray()
            self._sample_rate = self.audio.fps

    def detect_silence(self, audio_clip=None) -> List[Tuple[float, float]]:
        """æ£€æµ‹é™éŸ³åŒºåŸŸ"""
        if audio_clip is None:
            audio_clip = self.audio

        if audio_clip is None:
            raise ValueError("è§†é¢‘æ²¡æœ‰éŸ³é¢‘è½¨é“")

        # è·å–éŸ³é¢‘æ•°æ®
        self._ensure_audio_array()
        audio_array = self._audio_array
        sample_rate = self._sample_rate

        # åˆ†æ®µåˆ†æ (æ¯ 0.1s ä¸€æ®µ)
        chunk_duration = 0.1
        chunk_samples = int(chunk_duration * sample_rate)

        silence_regions = []
        in_silence = False
        silence_start = 0

        for i in range(0, len(audio_array), chunk_samples):
            chunk = audio_array[i:i+chunk_samples]
            if len(chunk) == 0:
                continue

            # è®¡ç®— RMS éŸ³é‡
            rms = np.sqrt(np.mean(chunk**2))
            db = 20 * np.log10(rms + 1e-10)

            if db < self.threshold:
                if not in_silence:
                    in_silence = True
                    silence_start = i / sample_rate
            else:
                if in_silence:
                    duration = (i / sample_rate) - silence_start
                    if duration >= self.min_gap:
                        silence_regions.append((silence_start, (i / sample_rate)))
                    in_silence = False

        # å¤„ç†ç»“å°¾çš„é™éŸ³
        if in_silence:
            duration = (len(audio_array) / sample_rate) - silence_start
            if duration >= self.min_gap:
                silence_regions.append((silence_start, len(audio_array) / sample_rate))

        # åˆå¹¶ç›¸è¿‘çš„æ°”å£
        merged = []
        for start, end in silence_regions:
            if merged and start - merged[-1][1] < self.merge_gap:
                merged[-1] = (merged[-1][0], end)
            else:
                merged.append((start, end))

        self._silence_regions = merged
        return merged

    def compute_gain_data(self) -> List[Dict]:
        """
        è®¡ç®—éŸ³é¢‘å¢ç›Šæ•°æ®ï¼ˆRMS å’Œå³°å€¼ï¼‰ï¼Œæ¯ 0.1s ä¸€ä¸ªé‡‡æ ·ç‚¹
        """
        self._ensure_audio_array()
        audio_array = self._audio_array
        sample_rate = self._sample_rate

        chunk_duration = 0.1
        chunk_samples = int(chunk_duration * sample_rate)

        gain_data = []
        for i in range(0, len(audio_array), chunk_samples):
            chunk = audio_array[i:i+chunk_samples]
            if len(chunk) == 0:
                continue

            t = i / sample_rate
            rms = np.sqrt(np.mean(chunk**2))
            peak = np.max(np.abs(chunk))

            rms_db = 20 * np.log10(rms + 1e-10)
            peak_db = 20 * np.log10(peak + 1e-10)

            gain_data.append({
                "time": round(t, 3),
                "rms_db": round(rms_db, 2),
                "peak_db": round(peak_db, 2),
            })

        self._gain_data = gain_data
        return gain_data

    def _compute_segment_ranges(self, silence_regions: List[Tuple[float, float]]) -> List[Tuple[float, float]]:
        """è®¡ç®—ä¿ç•™ç‰‡æ®µçš„æ—¶é—´èŒƒå›´"""
        segments = []
        prev_end = 0.0

        for start, end in silence_regions:
            if start - prev_end > 0.3:
                segments.append((prev_end, start))
            prev_end = end

        # æœ€åä¸€æ®µ
        if self.video.duration - prev_end > 0.3:
            segments.append((prev_end, self.video.duration))

        self._segments_ranges = segments
        return segments

    def cut_video(self, silence_regions: List[Tuple[float, float]]) -> List[VideoFileClip]:
        """æ ¹æ®é™éŸ³åŒºåŸŸå‰ªåˆ‡è§†é¢‘"""
        if self.video is None:
            self.load_video()

        seg_ranges = self._compute_segment_ranges(silence_regions)
        clips = []
        for s, e in seg_ranges:
            clips.append(self.video.subclip(s, e))

        return clips

    def analyze(self) -> dict:
        """åˆ†æè§†é¢‘æ°”å£"""
        if self.video is None:
            self.load_video()

        silence = self.detect_silence()
        self.compute_gain_data()

        total_silence = sum(end - start for start, end in silence)

        result = {
            "duration": self.video.duration,
            "silence_count": len(silence),
            "total_silence": total_silence,
            "cut_duration": self.video.duration - total_silence,
            "silence_regions": silence
        }

        print(f"\n[AutoCut] åˆ†æç»“æœ:")
        print(f"  è§†é¢‘æ—¶é•¿: {result['duration']:.2f}s")
        print(f"  æ°”å£æ•°é‡: {result['silence_count']}")
        print(f"  æ°”å£æ€»é•¿: {result['total_silence']:.2f}s")
        print(f"  é¢„è®¡å‰ªè¾‘å: {result['cut_duration']:.2f}s")

        if silence:
            print(f"\n  æ°”å£ä½ç½®:")
            for i, (s, e) in enumerate(silence, 1):
                print(f"    {i}. {s:.2f}s - {e:.2f}s ({e-s:.2f}s)")

        return result

    def show_preview(self, silence_regions: List[Tuple[float, float]],
                     save_image: bool = True):
        """
        å±•ç¤ºå‰ªè¾‘åé¢„è§ˆæ—¶é—´è½´
        - ç»ˆç«¯ ASCII é¢„è§ˆ
        - å¯é€‰ä¿å­˜é«˜æ¸…æ—¶é—´è½´å›¾
        """
        if self._segments_ranges is None:
            self._compute_segment_ranges(silence_regions)
        if self._gain_data is None:
            self.compute_gain_data()

        preview = TimelinePreview(
            total_duration=self.video.duration,
            silence_regions=silence_regions,
            segments=self._segments_ranges,
            gain_data=self._gain_data,
            threshold_db=self.threshold,
        )

        # ç»ˆç«¯é¢„è§ˆ
        preview.print_terminal_preview()

        # é«˜æ¸…å›¾é¢„è§ˆ
        if save_image:
            self._ensure_audio_array()
            img_name = Path(self.video_path).stem + "_timeline.png"
            img_path = str(Path(self.video_path).parent / img_name)
            preview.save_timeline_image(
                output_path=img_path,
                audio_array=self._audio_array,
                sample_rate=self._sample_rate,
            )

    def export(self, output_path: str, visualize: bool = False,
               preview: bool = True):
        """å¯¼å‡ºå‰ªè¾‘åçš„è§†é¢‘"""
        if self.video is None:
            self.load_video()

        silence = self.detect_silence()
        segments = self.cut_video(silence)

        if not segments:
            raise ValueError("æ²¡æœ‰å¯å¯¼å‡ºçš„ç‰‡æ®µ")

        print(f"[AutoCut] ç”Ÿæˆ {len(segments)} ä¸ªç‰‡æ®µ...")

        # æ‹¼æ¥ç‰‡æ®µ
        final = concatenate_videoclips(segments, method="compose")

        # å¯¼å‡º
        final.write_videofile(
            output_path,
            codec="libx264",
            audio_codec="aac",
            fps=24,
            preset="medium"
        )

        print(f"[AutoCut] å·²å¯¼å‡º: {output_path}")

        # æ°”å£æ³¢å½¢å¯è§†åŒ–ï¼ˆæ—§åŠŸèƒ½ï¼‰
        if visualize:
            self.visualize(silence)

        # â˜… æ–°å¢ï¼šå‰ªè¾‘åé¢„è§ˆæ—¶é—´è½´
        if preview:
            self.show_preview(silence, save_image=True)

        # æ¸…ç†
        for segment in segments:
            segment.close()
        final.close()
        self.video.close()

    def visualize(self, silence_regions: List[Tuple[float, float]]):
        """å¯è§†åŒ–éŸ³é¢‘æ³¢å½¢å’Œæ°”å£ä½ç½®ï¼ˆæ—§ç‰ˆç®€å•å›¾ï¼‰"""
        if not MOVIEPY_AVAILABLE:
            print("[AutoCut] å¯è§†åŒ–éœ€è¦ matplotlib")
            return

        self._ensure_audio_array()
        audio_array = self._audio_array
        sample_rate = self._sample_rate

        # ç»˜åˆ¶æ³¢å½¢
        fig, ax = plt.subplots(figsize=(14, 4))

        time = np.arange(len(audio_array)) / sample_rate

        # é™ä½é‡‡æ ·ç‡ä»¥åŠ å¿«ç»˜å›¾
        downsample = 100
        time = time[::downsample]
        audio_ds = audio_array[::downsample]

        ax.plot(time, audio_ds, linewidth=0.1, alpha=0.7)

        # æ ‡è®°æ°”å£
        for start, end in silence_regions:
            ax.axvspan(start, end, alpha=0.3, color='red',
                       label='æ°”å£' if start == silence_regions[0][0] else '')

        ax.set_xlabel('æ—¶é—´ (s)')
        ax.set_ylabel('éŸ³é‡')
        ax.set_title(f'éŸ³é¢‘æ³¢å½¢ä¸æ°”å£æ£€æµ‹ (é˜ˆå€¼: {self.threshold}dB)')
        ax.legend()

        plt.tight_layout()
        plt.savefig('autocut_waveform.png', dpi=100)
        print("[AutoCut] æ³¢å½¢å›¾å·²ä¿å­˜: autocut_waveform.png")

    def batch_process(self, input_dir: str, output_dir: str):
        """æ‰¹é‡å¤„ç†ç›®å½•ä¸‹çš„è§†é¢‘"""
        input_path = Path(input_dir)
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        video_files = list(input_path.glob("*.mp4")) + \
                      list(input_path.glob("*.mov")) + \
                      list(input_path.glob("*.mkv"))

        print(f"[AutoCut] æ‰¾åˆ° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")

        for video_file in video_files:
            print(f"\nå¤„ç†: {video_file.name}")
            self.video_path = str(video_file)
            self.load_video()

            output_file = output_path / f"{video_file.stem}_cut{video_file.suffix}"

            try:
                self.analyze()
                self.export(str(output_file))
            except Exception as e:
                print(f"  é”™è¯¯: {e}")


def main():
    parser = argparse.ArgumentParser(description='AutoCut - è‡ªåŠ¨å‰ªè¾‘æ°”å£')
    parser.add_argument('video', nargs='?', help='è§†é¢‘æ–‡ä»¶è·¯å¾„')
    parser.add_argument('-t', '--threshold', type=float, default=-30,
                       help='é™éŸ³é˜ˆå€¼ dB (é»˜è®¤: -30)')
    parser.add_argument('-g', '--min-gap', type=float, default=0.1,
                       help='æœ€å°æ°”å£æ—¶é•¿ ç§’ (é»˜è®¤: 0.1)')
    parser.add_argument('-m', '--merge', type=float, default=0.3,
                       help='åˆå¹¶é—´éš” ç§’ (é»˜è®¤: 0.3)')
    parser.add_argument('-o', '--output', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('-a', '--analyze-only', action='store_true',
                       help='ä»…åˆ†æï¼Œä¸å‰ªè¾‘')
    parser.add_argument('-v', '--visualize', action='store_true',
                       help='ç”Ÿæˆæ³¢å½¢å¯è§†åŒ–')
    parser.add_argument('--no-preview', action='store_true',
                       help='ç¦ç”¨å‰ªè¾‘åé¢„è§ˆæ—¶é—´è½´')
    parser.add_argument('--preview-only', action='store_true',
                       help='ä»…ç”Ÿæˆé¢„è§ˆæ—¶é—´è½´ï¼ˆä¸å¯¼å‡ºè§†é¢‘ï¼‰')
    parser.add_argument('--batch', help='æ‰¹é‡å¤„ç†ç›®å½•')

    args = parser.parse_args()

    if args.batch:
        cutter = AutoCut("", args.threshold, args.min_gap, args.merge)
        cutter.batch_process(args.batch, args.output or args.batch + "_output")
        return

    if not args.video:
        parser.print_help()
        return

    cutter = AutoCut(args.video, args.threshold, args.min_gap, args.merge)

    if args.preview_only:
        # ä»…é¢„è§ˆæ¨¡å¼ï¼šåˆ†æ + æ˜¾ç¤ºæ—¶é—´è½´
        cutter.load_video()
        silence = cutter.detect_silence()
        cutter._compute_segment_ranges(silence)
        cutter.show_preview(silence, save_image=True)
    elif args.analyze_only:
        cutter.load_video()
        result = cutter.analyze()
        # åˆ†ææ¨¡å¼ä¹Ÿæ˜¾ç¤ºé¢„è§ˆ
        silence = result['silence_regions']
        cutter._compute_segment_ranges(silence)
        cutter.show_preview(silence, save_image=False)
    else:
        output = args.output
        if not output:
            base = Path(args.video).stem
            output = f"{base}_cut.mp4"

        cutter.export(output, visualize=args.visualize,
                      preview=not args.no_preview)


if __name__ == "__main__":
    main()
